{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import sagemaker.session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = session.default_bucket()\n",
    "model_package_group_name = f\"CreditRiskModelPackageGroupName\"\n",
    "prefix = 'sagemaker/credit-xgboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the raw dataset\n",
    "input_data_uri = session.upload_data(path='dataset/UCI_Credit_Card.csv', key_prefix=prefix+'/data')\n",
    "print('Data set uploaded to ', input_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline input parameters\n",
    "\n",
    "Pipelines can be initiated with default parameters, but also injected when calling the pipeline.start() method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "# How many instances to use when processing\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "# What instance type to use for processing\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "# What instance type to use for training\n",
    "training_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "\n",
    "# Where the input data is stored\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_uri,\n",
    ")\n",
    "\n",
    "# What is the default status of the model when registering with model registry.\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Step: Pre process data (step_preprocess_data)\n",
    "In the first step, we create an sklearn processor use it in the process_step. We also create a caching configuration we provide to each step to cache the results for a given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep, CacheConfig\n",
    "\n",
    "# Create cache configuration\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"T30m\")\n",
    "\n",
    "# Create SKlean processor object\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.20.0\",\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"credit-processing-job\"\n",
    ")\n",
    "\n",
    "# Use the sklearn_processor in a Sagemaker pipelines ProcessingStep\n",
    "step_preprocess_data = ProcessingStep(\n",
    "    name=\"PreprocessCreditData\",\n",
    "    processor=sklearn_processor,\n",
    "    cache_config=cache_config,\n",
    "    inputs=[\n",
    "      ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),  \n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/output/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/output/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/output/test\"),\n",
    "        ProcessingOutput(output_name=\"baseline_with_headers\", source=\"/opt/ml/processing/output/baseline\")\n",
    "    ],\n",
    "    code=\"preprocessing.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Step: Train a model (step_train_model)\n",
    "In the second step, we use the train and validation output from the precious processing step.\n",
    "\n",
    "We retrieve the XGBoost container, create an XGBoost estimator, specify hyper parameters, and create the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# Where to store the trained model\n",
    "model_path = f\"s3://{bucket}/CreditTrain\"\n",
    "\n",
    "# Fetch container to use for training\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.2-2\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "# Create XGBoost estimator object\n",
    "xgb_estimator = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    output_path=model_path,\n",
    "    role=role,\n",
    "    disable_profiler=True,\n",
    ")\n",
    "\n",
    "# Specify hyperparameters\n",
    "xgb_estimator.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=25)\n",
    "\n",
    "# Use the xgb_estimator in a Sagemaker pipelines ProcessingStep. \n",
    "# NOTE how the input to the training job directly references the output of the previous step.\n",
    "step_train_model = TrainingStep(\n",
    "    name=\"TrainCreditModel\",\n",
    "    estimator=xgb_estimator,\n",
    "    cache_config=cache_config,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_preprocess_data.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_preprocess_data.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Step: Evaluate model (evaluate_model_processor)\n",
    "To evaluate the model we just trained, we need to write an evalutation script that we run in a processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluation.py\n",
    "import json\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = f\"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "    \n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "\n",
    "    test_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    df = pd.read_csv(test_path, header=None)\n",
    "    \n",
    "    y_test = df.iloc[:, 0].to_numpy()\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    \n",
    "    X_test = xgboost.DMatrix(df.values)\n",
    "    \n",
    "    prediction_probabilities = model.predict(X_test)\n",
    "    predictions = np.round(prediction_probabilities)\n",
    "    \n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    fpr, tpr, _ = roc_curve(y_test, prediction_probabilities)\n",
    "\n",
    "    \n",
    "    print('Accuracy: ', accuracy)\n",
    "    print('Precision: ', precision)\n",
    "    print('Recall: ', recall)\n",
    "    print('Confusion matrix: ', conf_matrix)\n",
    "\n",
    "    # Available metrics to add to model: https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html\n",
    "    report_dict = {\n",
    "        \"binary_classification_metrics\": {\n",
    "            \"accuracy\": {\n",
    "                \"value\": accuracy,\n",
    "                \"standard_deviation\" : \"NaN\"\n",
    "            },\n",
    "            \"precision\": {\n",
    "                \"value\": precision,\n",
    "                \"standard_deviation\" : \"NaN\"\n",
    "            },\n",
    "            \"recall\": {\n",
    "                \"value\": recall,\n",
    "                \"standard_deviation\" : \"NaN\"\n",
    "            },\n",
    "            \"confusion_matrix\" : {\n",
    "              \"0\" : {\n",
    "                \"0\" : int(conf_matrix[0][0]),\n",
    "                \"1\" : int(conf_matrix[0][1])\n",
    "              },\n",
    "              \"1\" : {\n",
    "                \"0\" : int(conf_matrix[1][0]),\n",
    "                \"1\" : int(conf_matrix[1][1])\n",
    "              },\n",
    "            },\n",
    "            \"receiver_operating_characteristic_curve\" : {\n",
    "              \"false_positive_rates\" : list(fpr),\n",
    "              \"true_positive_rates\" : list(tpr)\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the evaluation script, we create a ScriptProcessor object and use it in a ProcessingStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "# Create ScriptProcessor object.\n",
    "evaluate_model_processor = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"script-credit-eval\",\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "# Create a PropertyFile\n",
    "# We use a PropertyFile to be able to reference outputs from a processing step, for instance to use in a condition step, which we'll see later on.\n",
    "# For more information, visit https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-propertyfile.html\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\"\n",
    ")\n",
    "\n",
    "# Use the evaluate_model_processor in a Sagemaker pipelines ProcessingStep. \n",
    "step_evaluate_model = ProcessingStep(\n",
    "    name=\"EvaluateCreditModel\",\n",
    "    processor=evaluate_model_processor,\n",
    "    cache_config=cache_config,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train_model.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_preprocess_data.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"evaluation.py\",\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Step: Create model (step_create_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sagemaker.model import Model\n",
    "#from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "#model = Model(\n",
    "#    image_uri=image_uri,\n",
    "#    model_data=step_train_model.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "#    sagemaker_session=session,\n",
    "#    role=role,\n",
    "#)\n",
    "\n",
    "\n",
    "#step_create_model = CreateModelStep(\n",
    "#    name=\"CreateCreditModel\",\n",
    "#    model=model,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Step: Register model (step_register_model)\n",
    "\n",
    "I don't think the model_metrics step is working..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics \n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            step_evaluate_model.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Crete a RegisterModel step, which registers your model with Sagemaker Model Registry.\n",
    "step_register_model = RegisterModel(\n",
    "    name=\"RegisterCreditModel\",\n",
    "    estimator=xgb_estimator,\n",
    "    model_data=step_train_model.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\", \"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Step: Compute Model Monitor data quality baseline (step_create_data_baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create Processor object using the model monitor image\n",
    "baseline_processor = sagemaker.processing.Processor(\n",
    "    base_job_name=\"credit-risk-baseline-processor\",\n",
    "    image_uri=sagemaker.image_uris.retrieve(framework='model-monitor', region='eu-west-1'),\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=processing_instance_type,\n",
    "    env = {\n",
    "        \"dataset_format\": \"{\\\"csv\\\": {\\\"header\\\": true} }\",\n",
    "        \"dataset_source\": \"/opt/ml/processing/sm_input\",\n",
    "        \"output_path\": \"/opt/ml/processing/sm_output\",\n",
    "        \"publish_cloudwatch_metrics\": \"Disabled\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create a Sagemaker Pipeline step, using the baseline_processor.\n",
    "step_create_data_baseline = ProcessingStep(\n",
    "    name=\"CreateModelQualityBaseline\",\n",
    "    processor=baseline_processor,\n",
    "    cache_config=cache_config,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_preprocess_data.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"baseline_with_headers\"\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/sm_input\",\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            source=\"/opt/ml/processing/sm_output\",\n",
    "            destination=\"s3://{}/{}/baseline\".format(bucket, prefix),\n",
    "            output_name=\"baseline_result\",\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Condition Step: Meets accuracy requirements? (cond_gte)\n",
    "Adding conditions to the pipeline is done with a ConditionStep.\n",
    "In this case, we only want to register the model with model registry, and compute a new data quality baseline for model monitoring, if the new model meets some quality condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "\n",
    "# Create Condition\n",
    "cond_gte = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step=step_evaluate_model,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"binary_classification_metrics.accuracy.value\"\n",
    "    ),\n",
    "    right=0.7\n",
    ")\n",
    "\n",
    "# Create a Sagemaker Pipelines ConditionStep, using the condition we just created.\n",
    "step_cond = ConditionStep(\n",
    "    name=\"AccuracyCondition\",\n",
    "    conditions=[cond_gte],\n",
    "    if_steps=[step_register_model],\n",
    "    else_steps=[], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Creation: Orchestrate all steps\n",
    "\n",
    "Now that we've created all steps we need in our pipeline, we need to orchestrate all the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# Create a Sagemaker Pipeline\n",
    "pipeline_name = f\"CreditPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type, \n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "    ],\n",
    "    steps=[step_preprocess_data, step_train_model, step_evaluate_model, step_create_data_baseline, step_cond],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit pipeline, and start it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Inspect pipeline definition.\n",
    "#json.loads(pipeline.definition())\n",
    "\n",
    "# Submit pipline\n",
    "pipeline.upsert(role_arn=role)\n",
    "\n",
    "# Execute pipeline\n",
    "execution = pipeline.start()\n",
    "\n",
    "# Wait for pipeline to finish\n",
    "execution.wait()\n",
    "\n",
    "# List the execution steps to check out the status and artifacts:\n",
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
